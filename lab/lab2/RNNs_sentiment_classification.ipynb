{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs_sentiment_classification",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9EMEXxtb2gqA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lab 2 : TEXT CLASSIFICATION USING MLP, CNN and RNN\n",
        "\n",
        "### Deep Learning  Course Offered by : Pavlos Protopapas, IACS, Harvard University.\n",
        "\n",
        "TAs: Srivatsan Srinivasan, Patrick Ohiomoba"
      ]
    },
    {
      "metadata": {
        "id": "JesjlGDmIMCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will try to implement a RNN and compare its performance with other baselines. We will use the task of IMDB movie review classification. A sentence can be thought of as a sequence of words which have semantic connections across time. By semantic connection, we mean that the words that occur earlier in the sentence influence the sentence's structure and meaning in the latter part of the sentence. There are also semantic connections backwards in a sentence, in an ideal case (in which we use RNNs from both directions and combine their outputs). But for the purpose of this tutorial, we are going to restrict ourselves to only uni-directional RNNs. "
      ]
    },
    {
      "metadata": {
        "id": "EWgRqV0lAgpc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "import numpy as np\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpiyvyQiAs5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SEEDING - **\n",
        "Important thing to do in many machine learning tasks which involve stochastic sampling(where random numbers are generated for different samples) is to do seeding so that the results are fairly reproducible.\n",
        "\n",
        "**WHY SEEDING ?** Most random number generators in computers are pseudo-random number generators i.e. they generate random numbers starting from a seed, but internally have a deterministic formula to calculate the next random number it generates and thus, if you fix your seed, the set of random numbers produced are the same in every run.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "b6oE6xofBX_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **STEP 1** : Load and visualize the data"
      ]
    },
    {
      "metadata": {
        "id": "5QpTzwZvBhhH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
        "vocabulary_size = 10000\n",
        "\n",
        "#We also want to have a finite length of reviews and not have to process really long sentences.\n",
        "max_review_length = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uv_THMxbDSyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For practical data science applications, we need to convert text into tokens since the machine understands only numbers and not really English words like humans can. As a simple example of tokenization, we can see a small example.\n",
        "\n",
        "Assume we have 5 sentences. This is how we tokenize them into numbers once we create a dictionary.\n",
        "\n",
        "1. I have books                       -                        [1, 4, 7]\n",
        "2. Interesting books are useful                     [10,2,9,8]\n",
        "3. I have computers                                        [1,4,6]\n",
        "4. Computers are interesting and useful     [6,9,11,10,8]\n",
        "5. Books and computers are both valuable. [7,10,2,9,13,12]\n",
        "\n",
        "Create tokens for vocabulary based on frequency of occurrence. Hence, we assign the following tokens\n",
        "\n",
        "I-1, books-2, computers-3, have-4, are-5, computers-6,books-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
        "\n",
        "Thankfully, in our dataset it is internally handled and each sentence is represented in such tokenized form."
      ]
    },
    {
      "metadata": {
        "id": "icfiuXaoBdUD",
        "colab_type": "code",
        "outputId": "5a6d4130-23c9-4d47-e595-b4ce7a1f731c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DfEBJe9vBVjO",
        "colab_type": "code",
        "outputId": "57fc0997-a769-480f-da0f-95223a133f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of reviews', len(X_train))\n",
        "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
        "print('First review', X_train[0])\n",
        "print('First label', y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews 25000\n",
            "Length of first and fifth review before padding 218 147\n",
            "First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "First label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U1LWcZExHp3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pad sequences in order to ensure that all inputs have same sentence length and dimensions.\n",
        "\n",
        "<b> DISCUSSION </b> : Aren't RNNs supposed to handle varying lengths of sentences ? Why is padding necessary ? Hint : Batching and GPUs"
      ]
    },
    {
      "metadata": {
        "id": "j1ibR_xWA5fK",
        "colab_type": "code",
        "outputId": "624c4346-7552-4284-d903-6f21f7d68fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of first and fifth review after padding 500 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SO00uQ7JvF6d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Is Accuracy the right metric to look at ? \n",
        "\n",
        "Discuss : In what cases is accuracy a good metric to measure classification models ? \n",
        "\n",
        "What other metrics are useful incase accuracy proves to be incompetent metric for our dataset ?  https://towardsdatascience.com/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mvzvoYnNvPm-",
        "colab_type": "code",
        "outputId": "790dd79b-63b0-4759-816e-34d8bd74bb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counts = dict(Counter(y_train))\n",
        "print('Number of zeroes : ', counts[0], ' and Number of ones : ', counts[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of zeroes :  12500  and Number of ones :  12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_X0l7QDvlWMK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MODEL 1(a) : FEEDFORWARD NETWORKS WITHOUT EMBEDDINGS \n",
        "\n",
        "Let us build a single layer feedforward net with 250 nodes.\n",
        "\n",
        "<b> EXERCISE </b> : Calculate the number of parameters involved in this network and implement a feedforward net to do classification without looking at cells below."
      ]
    },
    {
      "metadata": {
        "id": "ZYaoE0fXkfcV",
        "colab_type": "code",
        "outputId": "e4c4266f-38cf-418f-ccf8-22bd0579911f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(250, activation='relu',input_dim=max_review_length))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 125,501\n",
            "Trainable params: 125,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            " - 1s - loss: 7.9558 - acc: 0.5006 - val_loss: 7.9719 - val_acc: 0.5000\n",
            "Epoch 2/2\n",
            " - 1s - loss: 7.9682 - acc: 0.5002 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Accuracy: 50.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UE1LtWH5lvWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Any idea why the performance is terrible ?\n",
        "\n",
        "Hint : Tokenization. \n",
        "\n",
        "Obvious Workaround : One-Hot Encodings"
      ]
    },
    {
      "metadata": {
        "id": "apqpdoncI0vl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### EMBEDDINGS - Sparse to Dense Transformations\n",
        "\n",
        "We use embeddings to reduce dimensions of our data since the tokens we assign based on our word frequency are discrete and do not have a continuous structure.\n",
        "\n",
        "#### What are embeddings ?\n",
        "\n",
        "Embeddings are functional transformations from a sparse discrete vector representation of text (either as tokens or as one-hot encodings) into a dense vector representation of a fixed size(usually of much lower dimensions than the vocabulary length of the text). The dense representations allow the neural network to generalize better.\n",
        "\n",
        "Here we are training our own embedding while training our neural network. To transfer \"knowledge\" from other sources, in more complicated projects we can also use pre-trained embeddings such as word-2-vec, GloVE, Fastext etc.  https://nlpforhackers.io/word-embeddings/"
      ]
    },
    {
      "metadata": {
        "id": "hrFJQQP4gG-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example Embeddings Transformation"
      ]
    },
    {
      "metadata": {
        "id": "qi8eHxEPfMJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us first understand what Keras embedding layer works through a dummy example to see how the dimensions are transformed. In this example, each input is mapped to a 64 dimensional vector.(via the embedding layer)\n",
        "\n",
        "<b>EXERCISE</b> : Manually calculate the number of parameters needed in the embedding layer before executing the code."
      ]
    },
    {
      "metadata": {
        "id": "tmojGnFcfSfC",
        "colab_type": "code",
        "outputId": "149f0cd8-49ba-4b76-f6bb-d1813bd63d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "#input - Number of categorical inputs, embedding dimension, input length.\n",
        "model.add(Embedding(1000, 64, input_length=10))\n",
        "print(model.summary())\n",
        "\n",
        "# the model will take as input an integer matrix of size (batch, input_length).\n",
        "# the largest integer (i.e. word index) in the input should be\n",
        "# no larger than 999 (vocabulary size).\n",
        "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "\n",
        "input_array = np.random.randint(1000, size=(32, 10))\n",
        "print(\"Shape of input : \", input_array.shape)\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "assert output_array.shape == (32, 10, 64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 10, 64)            64000     \n",
            "=================================================================\n",
            "Total params: 64,000\n",
            "Trainable params: 64,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Shape of input :  (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E_hDBZ_rflhW",
        "colab_type": "code",
        "outputId": "e756792a-f6a1-4835-f8c4-dc0e490c6f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(input_array[0])\n",
        "print(output_array[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[908  72 767 905 715 645 847 960 144 129]\n",
            "(10, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HXA43a_sgChz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MODEL 1(b) : FEEDFORWARD NETWORKS WITH EMBEDDINGS \n",
        "\n",
        "<b>EXERCISE</b> : Implement the feedforward net combining the embedding layer and the feedforward layer(one layer, 250 nodes) without looking at cells below. Manually calculate the number of parameters needed in the feedforward network before executing the code. "
      ]
    },
    {
      "metadata": {
        "id": "ZaOes1ab3Fs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ODxwyYlYIyNb",
        "colab_type": "code",
        "outputId": "44af240d-4855-4265-d404-2cb5d658c28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 500, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 50000)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 250)               12500250  \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 13,500,501\n",
            "Trainable params: 13,500,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "09CcheXuKfPn",
        "colab_type": "code",
        "outputId": "909b1c33-9b47-410d-bb38-ccc5d33a134f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            " - 9s - loss: 0.5295 - acc: 0.7073 - val_loss: 0.3109 - val_acc: 0.8663\n",
            "Epoch 2/2\n",
            " - 6s - loss: 0.1719 - acc: 0.9363 - val_loss: 0.2975 - val_acc: 0.8755\n",
            "Accuracy: 87.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sEffI6gOK8cY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MODEL 2 : Convolutional Nets\n",
        "\n",
        "Text can be thought of as 1-dimensional sequence and we can apply 1-D Convolutions over a set of words. Let us walk through convolutions on text data with this blog.\n",
        "\n",
        "http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/\n",
        "\n",
        "<b>EXERCISE</b> : Manually calculate the number of parameters needed in the feedforward network before executing the code."
      ]
    },
    {
      "metadata": {
        "id": "ApWcBG8NK_2Q",
        "colab_type": "code",
        "outputId": "637b1f63-6791-4f99-e738-6cefe99a493d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=embedding_dim, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 500, 100)          30100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 250, 100)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 25000)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 250)               6250250   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 7,280,601\n",
            "Trainable params: 7,280,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BWFN54kBLgpC",
        "colab_type": "code",
        "outputId": "0cd9ee7f-36a3-41dd-ead2-5306dca4f939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            " - 11s - loss: 0.4930 - acc: 0.7196 - val_loss: 0.2841 - val_acc: 0.8814\n",
            "Epoch 2/2\n",
            " - 8s - loss: 0.1980 - acc: 0.9245 - val_loss: 0.2756 - val_acc: 0.8855\n",
            "Accuracy: 88.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9EcCvXfjVxH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### EXERCISE\n",
        "\n",
        "Try other CNNs with\n",
        "\n",
        "1. Different kernel sizes\n",
        "2. Different pooling operations(AveragePooling1D)\n",
        "\n",
        "<b>DISCUSSION</b> : What does max and average pooling mean in terms of processing text sequences ? "
      ]
    },
    {
      "metadata": {
        "id": "xO4YpO-oL0yK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model 3 : Recurrent Neural Nets (RNN)\n",
        "\n",
        "A simple recurrent neural network does not have any fancy gates. To process any hidden input at time t, it uses input at time t and the hidden layer at time t-1. (Will explain through RNN slides) .\n",
        "\n",
        "Also let us walk through the parameters a little bit with the manual https://keras.io/layers/recurrent/"
      ]
    },
    {
      "metadata": {
        "id": "CfjHFEvUL3vR",
        "colab_type": "code",
        "outputId": "300f26d3-c6f1-4322-85dc-dbb64ed7755b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
        "model.add(SimpleRNN(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,020,201\n",
            "Trainable params: 1,020,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r32-I7gzMFjB",
        "colab_type": "code",
        "outputId": "400d16e3-530e-4724-9bdd-964a02dd7ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 140s 6ms/step - loss: 0.6233 - acc: 0.6434\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 140s 6ms/step - loss: 0.4843 - acc: 0.7719\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 140s 6ms/step - loss: 0.4323 - acc: 0.8097\n",
            "Accuracy: 77.20%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m7imMiQlW2vj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model 4 :  LSTMs (More advanced RNNs)\n",
        "\n",
        "Long Short Term Memoery Networks. Gradients and information are better propagated through the network through the use of certain gates. Helps mitigate vanishing gradients and exploding gradients.\n",
        "\n",
        "For interested readers, http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "metadata": {
        "id": "J0sC7KxrWs94",
        "colab_type": "code",
        "outputId": "a0114bec-4cfc-4349-a97a-4107717242d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 500, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,080,501\n",
            "Trainable params: 1,080,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yN6o7Pr5XhqB",
        "colab_type": "code",
        "outputId": "ca937162-c2ec-4112-bc49-a63e7efe2b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 407s 16ms/step - loss: 0.4220 - acc: 0.8075\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 406s 16ms/step - loss: 0.2840 - acc: 0.8875\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 405s 16ms/step - loss: 0.1940 - acc: 0.9268\n",
            "Accuracy: 87.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9VuZHIjS3H7T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### EXERCISE\n",
        "\n",
        "Try other RNNs with\n",
        "\n",
        "1. Different number of hidden units\n",
        "2. Different number of layers(Add more layers)\n",
        "3. Different activation functions"
      ]
    },
    {
      "metadata": {
        "id": "tpzuH9ZE49kP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## OPTIONAL CONTENT\n",
        "\n",
        "### Model 4 : LSTM with dropouts\n",
        "\n",
        "What are dropouts and why dropouts ?\n",
        "\n",
        "**Overfitting ** - A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data. \n",
        "\n",
        "**Solution** - Dropout - Randomly selected neurons are ignored during training. The contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
        "\n",
        "Not essential for the lab, but for interested readers - https://arxiv.org/pdf/1409.2329.pdf"
      ]
    },
    {
      "metadata": {
        "id": "W9cNDEWl3Yqw",
        "colab_type": "code",
        "outputId": "88fd6155-dbac-4f32-b8de-83f6e3d5f189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
        "model.add(LSTM(100, dropout = 0.2, recurrent_dropout=0.2))\n",
        "#model.add(Bidirectional(LSTM(100, dropout = 0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 500, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,080,501\n",
            "Trainable params: 1,080,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kQdpiN_ZTCPO",
        "colab_type": "code",
        "outputId": "b728d767-b7d1-4c6d-d808-8d8dd99a1663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 461s 18ms/step - loss: 0.4897 - acc: 0.7662\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 458s 18ms/step - loss: 0.3646 - acc: 0.8501\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 459s 18ms/step - loss: 0.3296 - acc: 0.8669\n",
            "Accuracy: 85.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B8jB6szpeytU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE**\n",
        "\n",
        "1. Try a GRU (Another modified RNN with gates) instead of a LSTM and see the performance difference.\n",
        "\n",
        "2. Try applying different dropouts and try training for more epochs.\n",
        "\n",
        "Note : There are still no consistent results between the performance difference of LSTM vs GRU."
      ]
    },
    {
      "metadata": {
        "id": "d6a3L2g-W906",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MODEL 5 : CNN + LSTM \n",
        "\n",
        "CNNs are good at learning spatial features and sentences can be thought of as 1-D spatial vectors (dimension being connotated by the sequence ordering among the words in the sentence.). We apply a LSTM over the features learned by the CNN (after a maxpooling layer). This leverages the power of CNNs and LSTMs combined. We expect the CNN to be able to pick out invariant features across the 1-D spatial structure(i.e. sentence) that characterize good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer followed by a feedforward for classification."
      ]
    },
    {
      "metadata": {
        "id": "yMVc-K8mW6wo",
        "colab_type": "code",
        "outputId": "3e85a74c-373b-4e8e-edcc-257b5ce6a4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 500, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 500, 32)           9632      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 200)               186400    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 1,196,233\n",
            "Trainable params: 1,196,233\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "69lBa7lFW8pD",
        "colab_type": "code",
        "outputId": "5d5f4507-f004-436e-f6a1-5d6e4739a37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 211s 8ms/step - loss: 0.4423 - acc: 0.7820\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 210s 8ms/step - loss: 0.2125 - acc: 0.9194\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 209s 8ms/step - loss: 0.1487 - acc: 0.9473\n",
            "Accuracy: 85.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GI2rAh6pfOeN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION\n",
        "\n",
        "We saw the power of sequence models and how they are useful in text classification. They give a solid performance, low memory footprint (thanks to shared parameters) and are able to understand and leverage the temporally connected information contained in the inputs. There is still an open debate about the performance vs memory benefits of CNNs vs RNNs in the research community.\n",
        "\n",
        "\n",
        "Credits : Keras Documentation, \n",
        "\n"
      ]
    }
  ]
}