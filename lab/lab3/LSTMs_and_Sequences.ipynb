{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMs and Sequences",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iGdPQh2lCNhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LAB 3 : LSTMs and Sequences\n",
        "\n",
        "#### Data Science course offered by Pavlos Protopapas\n",
        "\n",
        "TAs : Patrick Ohiomoba, Srivatsan Srinivasan\n",
        "\n",
        "In this lab, we will look at LSTMs and the power of LSTMs in modeling sequences. We will dive deep into two exercises - a.) using LSTMs to tag some named entities in a sentence ( such as person, geographic location etc.) and b.) learning the powerful seq-to-seq models to perform simple addition of 3 digit numbers. While it is important to learn the modeling part from these exercises, students will also benefit from understanding how data is parsed into formats conducive for using as I/O in Keras models in both these exercises.\n",
        "\n",
        "*  For a greater understanding of LSTMs and how they are different from simple RNNs, please refer to this blog. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "*  For a greater understanding of GRUs and how they are different from simple RNNs, please refer to this blog. https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be"
      ]
    },
    {
      "metadata": {
        "id": "Dbz_paN21cdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EXERCISE 1 : ENTITY TAGGING IN SENTENCES\n",
        "\n",
        "IOB tagging for parts of speech -\n",
        "https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"
      ]
    },
    {
      "metadata": {
        "id": "7jsHFGXmDuKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ]
    },
    {
      "metadata": {
        "id": "NHOU01g84COT",
        "colab_type": "code",
        "outputId": "f73ec22c-5be7-45e6-e6e1-52c1be6ae99f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "#RUN THIS ONLY IF YOU RUN ON GOOGLE COLAB\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9eae2cfe-06ce-4800-b048-dcc81a67ed56\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9eae2cfe-06ce-4800-b048-dcc81a67ed56\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ner_dataset.csv to ner_dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7AZ_37av5lK-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['ner_dataset.csv']),encoding='latin1')\n",
        "#df2 = pd.read_csv('ner_dataset.csv',encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w7h6PO1KNJ_t",
        "colab_type": "code",
        "outputId": "b44e9d0b-45f2-4154-a78d-15d9c0a1018a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "df2 = df2.fillna(method=\"ffill\")\n",
        "df2.tail(10)\n",
        "words = list(set(df2[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "tags = list(set(df2[\"Tag\"].values))\n",
        "n_words, n_tags = len(words), len(tags)\n",
        "print('Number of Words : ', len(words), ' and number of tags : ', len(tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words :  35179  and number of tags :  17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0lpQhDm-bMET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "C8NSxOskD0Ta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "In the preprocessing step, we are grouping our dataset into sentences. In each sentence, we are mapping each element to a triplet of (Word, POS, Tag(what we intend to learn)).\n",
        "\n",
        "As an example at the end of preprocessing, the output looks so. \n",
        "\n",
        "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]"
      ]
    },
    {
      "metadata": {
        "id": "SAtYygl9Ng-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCCGFLCdNjNc",
        "colab_type": "code",
        "outputId": "43b172b4-6b17-4168-9dac-1d103e34fba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "getter = SentenceGetter(df2)\n",
        "print(getter.get_next())\n",
        "sentences = getter.sentences\n",
        "\n",
        "max_len = 50\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Exdyj1WmB-6E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us pad sequences to be of same length"
      ]
    },
    {
      "metadata": {
        "id": "UhlL4yENNxVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
        "\n",
        "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLmVWZxON5Xd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nm_H1bX2Brr0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Let us build a bidirectional LSTM here. \n",
        "\n",
        "<b>Discussion</b>  : Why bidirectional LSTM is useful for this sequence tagging exercise ?\n",
        "\n",
        "\n",
        "Before building a LSTM, we need to learn a Keras wrapper called TimeDistributed which is useful to apply any operation in a LSTM module across timesteps."
      ]
    },
    {
      "metadata": {
        "id": "1rvlGlEc2mrl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### TimeDistributed Example\n",
        "\n",
        "TimeDistributed is a wrapper function call that applies an input operation on all the timesteps of an input data.  For instance I have a feedforward network which converts a 10-dim vector to a 5-dim vector, then wrapping this timedistributed layer on that feedforward operation would convert a batch_size  \\* sentence_len \\* vector_len(=10) to batch_size  \\* sentence_len \\*  output_len(=5)"
      ]
    },
    {
      "metadata": {
        "id": "hJDJG1o13Zyl",
        "colab_type": "code",
        "outputId": "25dae9cc-b62d-4dde-e36c-affa0b05c4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "#Inputs to it will be batch_size*time_steps*input_vector_dim(to Dense) . Output will be batch_size*time_steps* output_vector_dim\n",
        "#Here dense converts a 5-dim input vector to a 8-dim vector.\n",
        "model.add(TimeDistributed(Dense(8), input_shape=(3, 5)))\n",
        "input_array = np.random.randint(10, size=(1,3,5))\n",
        "print(\"Shape of input : \", input_array.shape)\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "print(\"Shape of output : \", output_array.shape)\n",
        "# note: `None` is the batch dimension\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of input :  (1, 3, 5)\n",
            "Shape of output :  (1, 3, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qg8f3J7ROBEZ",
        "colab_type": "code",
        "outputId": "8935dba0-8c80-4198-e36c-16a14d97d914",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from keras.models import Model, Input\n",
        "import numpy as np\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "\n",
        "input = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
        "model = Dropout(0.1)(model)\n",
        "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
        "\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=5, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 38846 samples, validate on 4317 samples\n",
            "Epoch 1/5\n",
            "38846/38846 [==============================] - 318s 8ms/step - loss: 0.1424 - acc: 0.9641 - val_loss: 0.0664 - val_acc: 0.9808\n",
            "Epoch 2/5\n",
            "38846/38846 [==============================] - 318s 8ms/step - loss: 0.0551 - acc: 0.9840 - val_loss: 0.0533 - val_acc: 0.9843\n",
            "Epoch 3/5\n",
            "38846/38846 [==============================] - 315s 8ms/step - loss: 0.0458 - acc: 0.9866 - val_loss: 0.0501 - val_acc: 0.9852\n",
            "Epoch 4/5\n",
            "38846/38846 [==============================] - 315s 8ms/step - loss: 0.0413 - acc: 0.9878 - val_loss: 0.0480 - val_acc: 0.9856\n",
            "Epoch 5/5\n",
            "38846/38846 [==============================] - 317s 8ms/step - loss: 0.0382 - acc: 0.9886 - val_loss: 0.0468 - val_acc: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yq-M9iheBnfH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us generate a sentence and look at what our model has learned."
      ]
    },
    {
      "metadata": {
        "id": "tSF-4XWvOY9W",
        "colab_type": "code",
        "outputId": "db03f96c-c1df-4acc-ad17-a67fb5bcabec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "cell_type": "code",
      "source": [
        "i = 2318\n",
        "p = model.predict(np.array([X_te[i]]))\n",
        "t = [tags[k] for k in np.argmax(y_te[i], axis=-1)]\n",
        "p = np.argmax(p, axis=-1)\n",
        "print(\"{:15} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
        "for w, true, pred in zip(X_te[i], t, p[0]):\n",
        "    print(\"{:15}: ({:5}): {}\".format(words[w], true, tags[pred]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word            (True ): Pred\n",
            "Iran           : (B-geo): B-geo\n",
            "angered        : (O    ): O\n",
            "Washington     : (B-geo): B-geo\n",
            "and            : (O    ): O\n",
            "the            : (O    ): O\n",
            "European       : (B-org): B-org\n",
            "Union          : (I-org): I-org\n",
            "by             : (O    ): O\n",
            "resuming       : (O    ): O\n",
            "uranium        : (O    ): O\n",
            "conversion     : (O    ): O\n",
            "this           : (O    ): O\n",
            "week           : (O    ): O\n",
            "after          : (O    ): O\n",
            "rejecting      : (O    ): O\n",
            "an             : (O    ): O\n",
            "EU             : (B-org): B-org\n",
            "offer          : (O    ): O\n",
            "of             : (O    ): O\n",
            "political      : (O    ): O\n",
            "and            : (O    ): O\n",
            "economic       : (O    ): O\n",
            "incentives     : (O    ): O\n",
            "in             : (O    ): O\n",
            "return         : (O    ): O\n",
            "for            : (O    ): O\n",
            "giving         : (O    ): O\n",
            "up             : (O    ): O\n",
            "its            : (O    ): O\n",
            "nuclear        : (O    ): O\n",
            "program        : (O    ): O\n",
            ".              : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n",
            "ENDPAD         : (O    ): O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lRhqAuuLhQ3h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EXERCISE 2 : LEARNING TO ADD TWO NUMBERS (<1000) USING LSTMs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lrWE2zkjidjv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import Dense, RepeatVector, TimeDistributed\n",
        "import numpy as np\n",
        "from six.moves import range\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "syb6B9EgijZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Encode and decode sequences \n",
        "\n",
        "What is one hot encoding ? \n",
        "\n",
        "One-hot encoding is an indicator encoding for mapping discrete tokens. It is done to remove \"ORDINALITY\" effects.\n",
        "\n",
        "Let us take an example. If I have to create one-hot encodings for characters from 0-9, the one-hot encoding of any character is .\n",
        "\n",
        "0 - [1,0,0,0,0,0,0,0,0,0]\n",
        "1 - [0,1,0,0,0,0,0,0,0,0]\n",
        "...\n",
        "9 - [0,0,0,0,0,0,0,0,0,1]\n",
        "\n",
        "Same as the dummy variable you use when employing categorical variables in regression models."
      ]
    },
    {
      "metadata": {
        "id": "X4q9oyLwih7l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharacterTable(object):\n",
        "    def __init__(self, chars):        \n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):        \n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):        \n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dSsQdsnbjYat",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us set some hyperparameters and create our character set."
      ]
    },
    {
      "metadata": {
        "id": "YXyrBsuNjfAq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "MAXOUTPUTLEN = DIGITS + 1\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_v_bRlWcml9N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us write some functions to generate and preprocess training examples. Mostly you will find comments inline that explain the procedure. Broadly, we generate two numbers(<1000) randomly add a +sign between them convert them into a string of length 7 and create one hot encoding of this problem."
      ]
    },
    {
      "metadata": {
        "id": "ukWNjn9Ojp5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def return_random_digit():\n",
        "  return np.random.choice(list('0123456789'))  \n",
        "  \n",
        "def generate_number():\n",
        "  num_digits = np.random.randint(1, DIGITS + 1)  \n",
        "  return int(''.join( return_random_digit()\n",
        "                      for i in range(num_digits)))\n",
        "\n",
        "def data_generate(num_examples):\n",
        "  questions = []\n",
        "  expected = []\n",
        "  seen = set()\n",
        "  print('Generating data...')\n",
        "  while len(questions) < TRAINING_SIZE:      \n",
        "      a, b = generate_number(), generate_number()  \n",
        "      #Remove already seen elements\n",
        "      key = tuple(sorted((a, b)))\n",
        "      if key in seen:\n",
        "          continue\n",
        "      seen.add(key)\n",
        "      # Pad the data with spaces such that it is always MAXLEN.\n",
        "      q = '{}+{}'.format(a, b)\n",
        "      query = q + ' ' * (MAXLEN - len(q))\n",
        "      ans = str(a + b)\n",
        "      # Answers can be of maximum size DIGITS + 1.\n",
        "      ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "      questions.append(query)\n",
        "      expected.append(ans)\n",
        "  print('Total addition questions:', len(questions))\n",
        "  return questions, expected\n",
        "\n",
        "\n",
        "def encode_examples(questions,answers):\n",
        "  x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "  y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "  for i, sentence in enumerate(questions):\n",
        "      x[i] = ctable.encode(sentence, MAXLEN)\n",
        "  for i, sentence in enumerate(answers):\n",
        "      y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "  indices = np.arange(len(y))\n",
        "  np.random.shuffle(indices)\n",
        "  return x[indices],y[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_BjgEifnK7B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generate data and make train test split. Also, let us take some time to visualize the data and interpret the dimensions."
      ]
    },
    {
      "metadata": {
        "id": "aWmr9jvfg1Ic",
        "colab_type": "code",
        "outputId": "be59444b-67cc-4545-a9fa-17d83beca746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "q,a = data_generate(TRAINING_SIZE)\n",
        "x,y = encode_examples(q,a)\n",
        "split_at = len(x) - len(x) // 10\n",
        "x_train, x_val, y_train, y_val = x[:split_at], x[split_at:],y[:split_at],y[split_at:]\n",
        "\n",
        "\n",
        "print('Training Data shape:')\n",
        "print('X : ', x_train.shape)\n",
        "print('Y : ', y_train.shape)\n",
        "\n",
        "print('Sample Question(in decoded form) : ', ctable.decode(x_train[0]),'Sample Output : ', ctable.decode(y_train[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total addition questions: 50000\n",
            "Training Data shape:\n",
            "X :  (45000, 7, 12)\n",
            "Y :  (45000, 4, 12)\n",
            "Sample Question(in decoded form) :  10+27   Sample Output :  37  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1EVVd2AGsvSv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "In the Model, we follow sequence to sequence approach i.e. we build an encoder RNN use its learned states in a decoder RNN. \n",
        "\n",
        "For making our lives easy while creating multiple copies of hidden state, here is a useful Keras function that we need to learn."
      ]
    },
    {
      "metadata": {
        "id": "-sehDjwP2bE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### RepeatVector Example \n",
        "\n",
        "Repeats the vector a specified number of times. Dimension changes from batch_size * number of elements to batch_size* number of repetitions * number of elements."
      ]
    },
    {
      "metadata": {
        "id": "E7uX_MJRtdA4",
        "colab_type": "code",
        "outputId": "7a30af98-b1fe-41cf-e44c-a2cfee82aa83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "#converts from 1*32 to 1 * 6\n",
        "model.add(Dense(6, input_dim=10))\n",
        "print(model.output_shape)\n",
        "#converts from 1*6 to 1*3*6\n",
        "model.add(RepeatVector(3))\n",
        "print(model.output_shape) \n",
        "input_array = np.random.randint(1000, size=(1, 10))\n",
        "print(\"Shape of input : \", input_array.shape)\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "print(\"Shape of output : \", output_array.shape)\n",
        "# note: `None` is the batch dimension\n",
        "print('Input : ', input_array[0])\n",
        "print('Output : ', output_array[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 6)\n",
            "(None, 3, 6)\n",
            "Shape of input :  (1, 10)\n",
            "Shape of output :  (1, 3, 6)\n",
            "Input :  [874 891  48 263  93 654 648 101 700 986]\n",
            "Output :  [[-345.1526     30.735317  -69.681915  357.6878    351.5093   -131.6774  ]\n",
            " [-345.1526     30.735317  -69.681915  357.6878    351.5093   -131.6774  ]\n",
            " [-345.1526     30.735317  -69.681915  357.6878    351.5093   -131.6774  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ux5EsGosnGsv",
        "colab_type": "code",
        "outputId": "cb55c9f7-b268-49fe-8b7f-befd19a8be7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "#Hyperaparams\n",
        "RNN = layers.LSTM\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "LAYERS = 1\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "#ENCODING\n",
        "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "model.add(RepeatVector(MAXOUTPUTLEN))\n",
        "#DECODING\n",
        "for _ in range(LAYERS):    \n",
        "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
        "\n",
        "model.add(TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_7 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IgzGRZ7j5Q6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train and validate\n",
        "\n",
        "After every 15 epochs, we are checking how the addition performs over 20 random examples from the validation set."
      ]
    },
    {
      "metadata": {
        "id": "5CWlZ44TsG2b",
        "colab_type": "code",
        "outputId": "53cf2e53-6dc7-4223-e8a8-1de2f56581e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1526
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for iteration in range(1, 2):\n",
        "    print()  \n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=20,\n",
        "              validation_data=(x_val, y_val))\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    print('Finished iteration ', iteration)\n",
        "    numcorrect = 0\n",
        "    numtotal = 20\n",
        "    \n",
        "    for i in range(numtotal):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = model.predict_classes(rowx, verbose=0)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Question', q, end=' ')\n",
        "        print('True', correct, end=' ')\n",
        "        print('Guess', guess, end=' ')\n",
        "        if guess == correct :\n",
        "          print('Good job')\n",
        "          numcorrect += 1\n",
        "        else:\n",
        "          print('Fail')\n",
        "         \n",
        "     print('The model scored ', numcorrect*100/numtotal,' % in its test.')\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "45000/45000 [==============================] - 12s 258us/step - loss: 0.0958 - acc: 0.9796 - val_loss: 0.0965 - val_acc: 0.9762\n",
            "Epoch 2/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0820 - acc: 0.9831 - val_loss: 0.0967 - val_acc: 0.9748\n",
            "Epoch 3/20\n",
            "45000/45000 [==============================] - 12s 263us/step - loss: 0.0696 - acc: 0.9868 - val_loss: 0.0724 - val_acc: 0.9828\n",
            "Epoch 4/20\n",
            "45000/45000 [==============================] - 12s 263us/step - loss: 0.0703 - acc: 0.9841 - val_loss: 0.0651 - val_acc: 0.9858\n",
            "Epoch 5/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0563 - acc: 0.9888 - val_loss: 0.0629 - val_acc: 0.9843\n",
            "Epoch 6/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0611 - acc: 0.9853 - val_loss: 0.1039 - val_acc: 0.9644\n",
            "Epoch 7/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0477 - acc: 0.9899 - val_loss: 0.0420 - val_acc: 0.9906\n",
            "Epoch 8/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0573 - acc: 0.9862 - val_loss: 0.0423 - val_acc: 0.9905\n",
            "Epoch 9/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0293 - acc: 0.9958 - val_loss: 0.0358 - val_acc: 0.9928\n",
            "Epoch 10/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0299 - acc: 0.9947 - val_loss: 0.0601 - val_acc: 0.9806\n",
            "Epoch 11/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0336 - acc: 0.9928 - val_loss: 0.0491 - val_acc: 0.9864\n",
            "Epoch 12/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0500 - acc: 0.9864 - val_loss: 0.1078 - val_acc: 0.9615\n",
            "Epoch 13/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0287 - acc: 0.9939 - val_loss: 0.0334 - val_acc: 0.9915\n",
            "Epoch 14/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0317 - acc: 0.9926 - val_loss: 0.0262 - val_acc: 0.9939\n",
            "Epoch 15/20\n",
            "45000/45000 [==============================] - 12s 261us/step - loss: 0.0161 - acc: 0.9977 - val_loss: 0.0275 - val_acc: 0.9922\n",
            "Epoch 16/20\n",
            "45000/45000 [==============================] - 12s 261us/step - loss: 0.0504 - acc: 0.9855 - val_loss: 0.0246 - val_acc: 0.9945\n",
            "Epoch 17/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0141 - acc: 0.9983 - val_loss: 0.0223 - val_acc: 0.9949\n",
            "Epoch 18/20\n",
            "45000/45000 [==============================] - 12s 262us/step - loss: 0.0291 - acc: 0.9930 - val_loss: 0.2253 - val_acc: 0.9284\n",
            "Epoch 19/20\n",
            "45000/45000 [==============================] - 12s 261us/step - loss: 0.0297 - acc: 0.9923 - val_loss: 0.0195 - val_acc: 0.9957\n",
            "Epoch 20/20\n",
            "45000/45000 [==============================] - 12s 260us/step - loss: 0.0109 - acc: 0.9987 - val_loss: 0.0195 - val_acc: 0.9951\n",
            "Finished iteration  1\n",
            "Question 892+14  True 906  Guess 906  Good job\n",
            "The model scored  5.0  % in its test.\n",
            "Question 322+454 True 776  Guess 776  Good job\n",
            "The model scored  10.0  % in its test.\n",
            "Question 394+381 True 775  Guess 785  Fail\n",
            "The model scored  10.0  % in its test.\n",
            "Question 27+892  True 919  Guess 919  Good job\n",
            "The model scored  15.0  % in its test.\n",
            "Question 39+43   True 82   Guess 82   Good job\n",
            "The model scored  20.0  % in its test.\n",
            "Question 33+481  True 514  Guess 514  Good job\n",
            "The model scored  25.0  % in its test.\n",
            "Question 86+427  True 513  Guess 513  Good job\n",
            "The model scored  30.0  % in its test.\n",
            "Question 641+3   True 644  Guess 644  Good job\n",
            "The model scored  35.0  % in its test.\n",
            "Question 506+73  True 579  Guess 579  Good job\n",
            "The model scored  40.0  % in its test.\n",
            "Question 13+141  True 154  Guess 154  Good job\n",
            "The model scored  45.0  % in its test.\n",
            "Question 630+59  True 689  Guess 689  Good job\n",
            "The model scored  50.0  % in its test.\n",
            "Question 3+939   True 942  Guess 942  Good job\n",
            "The model scored  55.0  % in its test.\n",
            "Question 865+785 True 1650 Guess 1650 Good job\n",
            "The model scored  60.0  % in its test.\n",
            "Question 146+33  True 179  Guess 179  Good job\n",
            "The model scored  65.0  % in its test.\n",
            "Question 513+258 True 771  Guess 771  Good job\n",
            "The model scored  70.0  % in its test.\n",
            "Question 995+501 True 1496 Guess 1496 Good job\n",
            "The model scored  75.0  % in its test.\n",
            "Question 626+198 True 824  Guess 824  Good job\n",
            "The model scored  80.0  % in its test.\n",
            "Question 856+2   True 858  Guess 858  Good job\n",
            "The model scored  85.0  % in its test.\n",
            "Question 861+147 True 1008 Guess 909  Fail\n",
            "The model scored  85.0  % in its test.\n",
            "Question 162+956 True 1118 Guess 1118 Good job\n",
            "The model scored  90.0  % in its test.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6v06GEK36jrj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### EXERCISE\n",
        "\n",
        "1. Try changing the hyperparams, use other RNNs, more layers, check if increasing the number of epochs is useful.\n",
        "\n",
        "2.  Try reversing the data from validation set and check if commutative property of addition is learned by the model. Try printing the hidden layer with two inputs that are commutative and check if the hidden representations it learned are same or similar. Do we expect it to be true ? If so, why ? If not why ? You can access the layer using an index with model.layers and layer.output will give the output of that layer.\n",
        "\n",
        "3. (TAKE-HOME Cannot be completed within the class) Try doing addition in the RNN model the same way we do by hand. Reverse the order of digits and at each time step, input two digits(units in the first time step, tens in the second time step etc.)\n"
      ]
    }
  ]
}